{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-tabnet"
      ],
      "metadata": {
        "id": "gG-rut8-zxz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "import pickle"
      ],
      "metadata": {
        "id": "9RHbE2p45Y5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "ShPlOcFEae5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "cvfaedsDrYUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "/content/drive/MyDrive/Colab Notebooks/vehicle_data/casim_output.xlsx"
      ],
      "metadata": {
        "id": "qr9Yrs9d5YxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "carsim = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/vehicle_data/casim_output.xlsx')"
      ],
      "metadata": {
        "id": "Cl2QFtPq5Yq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matlab = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/vehicle_data/matlab_output_compare_14dof - 최종본.xlsx')"
      ],
      "metadata": {
        "id": "n__jdsE608wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_rows = pd.merge(carsim[['car_name']], matlab[['car_name']], on='car_name')['car_name']\n",
        "carsim_common = carsim[carsim['car_name'].isin(common_rows)]\n",
        "matlab_common = matlab[matlab['car_name'].isin(common_rows)]"
      ],
      "metadata": {
        "id": "_3Za4WYF31g2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "carsim_common['SSG'].describe()"
      ],
      "metadata": {
        "id": "zZ91EK5uTOR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matlab_common['SSG'].describe()"
      ],
      "metadata": {
        "id": "7d84BujuTS-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(carsim_common['SSG'])\n",
        "sns.scatterplot(matlab_common['SSG'])"
      ],
      "metadata": {
        "id": "gE-LU8psTg7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(carsim_common[carsim_common['car_name'] == 'AVANTE']['SSG'] - matlab_common[matlab_common['car_name'] == 'AVANTE']['SSG'])"
      ],
      "metadata": {
        "id": "Ee0jZMeoSxKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(carsim_common[carsim_common['car_name'] == 'X5']['SSG'] - matlab_common[matlab_common['car_name'] == 'X5']['SSG'])"
      ],
      "metadata": {
        "id": "EB25HzwHTDHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " sns.scatterplot(carsim_common[carsim_common['car_name'] == 'GRANDUER']['SSG'] - matlab_common[matlab_common['car_name'] == 'GRANDUER']['SSG'])"
      ],
      "metadata": {
        "id": "tjRzuJg160C0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "carsim_common['car_name'].unique()"
      ],
      "metadata": {
        "id": "KwLSoc6r5RYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "carsim_test = carsim[(carsim['vehicle_sprung_mass'] >= 1500) & (carsim['vehicle_sprung_mass'] < 1600)]\n",
        "carsim_train = carsim[~((carsim['vehicle_sprung_mass'] >= 1500) & (carsim['vehicle_sprung_mass'] <= 1600))]\n",
        "\n",
        "carsim_x_train = carsim_train.iloc[:, 3:]\n",
        "carsim_y_train = carsim_train.iloc[:, :3]\n",
        "carsim_x_test = carsim_test.iloc[:, 3:]\n",
        "carsim_y_test = carsim_test.iloc[:, :3]"
      ],
      "metadata": {
        "id": "5LCEzfelN-1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "750, 1110, 1270, 1370, 1590"
      ],
      "metadata": {
        "id": "WfoWT2a6ChYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "carsim750 = carsim[(carsim['car_name'] == 'A_HATCHBACK')].iloc[0, :]"
      ],
      "metadata": {
        "id": "c9m41-5wCoD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "carsim1110 = carsim[(carsim['car_name'] == 'B_HATCHBACK')].iloc[1, :]\n",
        "carsim1270 = carsim[(carsim['car_name'] == 'C_HATCHBACK')].iloc[3, :]\n",
        "carsim1370 = carsim[(carsim['car_name'] == 'D_SEDAN')].iloc[4, :]\n",
        "carsim1590 = carsim[(carsim['car_name'] == 'E_SUV')].iloc[12, :]"
      ],
      "metadata": {
        "id": "020MjGfvC99O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "carsim_testset = pd.concat([carsim750, carsim1110, carsim1270, carsim1370, carsim1590], axis = 1).T"
      ],
      "metadata": {
        "id": "VOBuewR5Exaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "carsim_trainset = carsim.drop(carsim_testset.index)"
      ],
      "metadata": {
        "id": "JSkseR8YFaVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "carsim.shape, matlab.shape"
      ],
      "metadata": {
        "id": "wzX2pqAD5Yn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class VehiclePerformancePredictor:\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.scaler = StandardScaler()\n",
        "        self.models = {\n",
        "            'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "            'XGBoost': xgb.XGBRegressor(objective='reg:squarederror', random_state=42),\n",
        "            'LightGBM': lgb.LGBMRegressor(random_state=42),\n",
        "            'SVM': SVR()\n",
        "        }\n",
        "        self.results = {}\n",
        "        self.predictions = {}  # 예측값을 저장할 딕셔너리\n",
        "        self.y_test_values = {}  # 실제 y_test 값을 저장할 딕셔너리\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        # Split the data\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
        "            self.X, self.y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Standardize the data\n",
        "        self.X_train = self.scaler.fit_transform(self.X_train)\n",
        "        self.X_test = self.scaler.transform(self.X_test)\n",
        "\n",
        "    def train_models(self):\n",
        "        # Train machine learning models\n",
        "        for target in self.y.columns:\n",
        "            y_train_target = self.y_train[target]\n",
        "            y_test_target = self.y_test[target]\n",
        "\n",
        "            self.y_test_values[target] = y_test_target  # 실제 y_test 값 저장\n",
        "\n",
        "            for model_name, model in self.models.items():\n",
        "                model.fit(self.X_train, y_train_target)\n",
        "                y_pred = model.predict(self.X_test)\n",
        "\n",
        "                # Evaluate the model\n",
        "                r2 = r2_score(y_test_target, y_pred)\n",
        "                mse = mean_squared_error(y_test_target, y_pred)\n",
        "                mae = mean_absolute_error(y_test_target, y_pred)\n",
        "\n",
        "                # Save the model\n",
        "                with open(f'{model_name}_{target}_model.pkl', 'wb') as file:\n",
        "                    pickle.dump(model, file)\n",
        "\n",
        "                # Save the results\n",
        "                self.results[f'{model_name}_{target}'] = {\n",
        "                    'R2': r2,\n",
        "                    'MSE': mse,\n",
        "                    'MAE': mae\n",
        "                }\n",
        "\n",
        "                # Save predictions\n",
        "                self.predictions[f'{model_name}_{target}'] = y_pred  # 예측값 저장\n",
        "\n",
        "        # Train Stacking model\n",
        "        estimators = [\n",
        "            ('rf', self.models['RandomForest']),\n",
        "            ('xgb', self.models['XGBoost']),\n",
        "            ('lgbm', self.models['LightGBM']),\n",
        "            ('svm', self.models['SVM'])\n",
        "        ]\n",
        "        stacking_model = StackingRegressor(estimators=estimators, final_estimator=LinearRegression())\n",
        "\n",
        "        for target in self.y.columns:\n",
        "            y_train_target = self.y_train[target]\n",
        "            y_test_target = self.y_test[target]\n",
        "\n",
        "            stacking_model.fit(self.X_train, y_train_target)\n",
        "            y_pred = stacking_model.predict(self.X_test)\n",
        "\n",
        "            # Evaluate the model\n",
        "            r2 = r2_score(y_test_target, y_pred)\n",
        "            mse = mean_squared_error(y_test_target, y_pred)\n",
        "            mae = mean_absolute_error(y_test_target, y_pred)\n",
        "\n",
        "            # Save the model\n",
        "            with open(f'Stacking_{target}_model.pkl', 'wb') as file:\n",
        "                pickle.dump(stacking_model, file)\n",
        "\n",
        "            # Save the results\n",
        "            self.results[f'Stacking_{target}'] = {\n",
        "                'R2': r2,\n",
        "                'MSE': mse,\n",
        "                'MAE': mae\n",
        "            }\n",
        "\n",
        "            # Save predictions\n",
        "            self.predictions[f'Stacking_{target}'] = y_pred  # 예측값 저장\n",
        "\n",
        "    def train_tabnet(self):\n",
        "        # Train TabNet model\n",
        "        tabnet_model = TabNetRegressor()\n",
        "        tabnet_model.fit(\n",
        "            X_train=self.X_train, y_train=self.y_train.values,\n",
        "            eval_set=[(self.X_test, self.y_test.values)],\n",
        "            patience=10, max_epochs=100,\n",
        "            eval_metric=['rmse'],\n",
        "            batch_size=16, virtual_batch_size=8\n",
        "        )\n",
        "\n",
        "        # Save the TabNet model\n",
        "        with open('TabNet_model.pkl', 'wb') as file:\n",
        "            pickle.dump(tabnet_model, file)\n",
        "\n",
        "        # Predict and evaluate for each target\n",
        "        y_pred = tabnet_model.predict(self.X_test)\n",
        "\n",
        "        for i, target in enumerate(self.y.columns):\n",
        "            r2 = r2_score(self.y_test[target], y_pred[:, i])\n",
        "            mse = mean_squared_error(self.y_test[target], y_pred[:, i])\n",
        "            mae = mean_absolute_error(self.y_test[target], y_pred[:, i])\n",
        "\n",
        "            # Save the results\n",
        "            self.results[f'TabNet_{target}'] = {\n",
        "                'R2': r2,\n",
        "                'MSE': mse,\n",
        "                'MAE': mae\n",
        "            }\n",
        "\n",
        "            # Save predictions\n",
        "            self.predictions[f'TabNet_{target}'] = y_pred[:, i]  # 예측값 저장\n",
        "\n",
        "    def get_results(self):\n",
        "        # Convert results to DataFrame\n",
        "        results_df = pd.DataFrame(self.results).T\n",
        "        return results_df\n",
        "\n",
        "    def get_predictions(self):\n",
        "        # 실제 값과 예측 값을 DataFrame으로 반환\n",
        "        predictions_df = {}\n",
        "        for target in self.y.columns:\n",
        "            predictions_df[target] = pd.DataFrame({\n",
        "                'y_test': self.y_test_values[target],\n",
        "                'Predicted': self.predictions.get(f'TabNet_{target}', 'N/A')\n",
        "            })\n",
        "        return predictions_df\n",
        "\n",
        "    def get_best_model_predictions(self):\n",
        "        # 각 target에 대해 가장 높은 R2 성능을 보이는 모델과 그 예측값을 반환\n",
        "        best_model_predictions = {}\n",
        "        for target in self.y.columns:\n",
        "            best_r2 = -float('inf')\n",
        "            best_model = None\n",
        "\n",
        "            # 각 모델의 성능을 확인하여 가장 높은 R2를 보이는 모델을 선택\n",
        "            for model_name in ['RandomForest', 'XGBoost', 'LightGBM', 'SVM', 'Stacking', 'TabNet']:\n",
        "                model_key = f'{model_name}_{target}'\n",
        "                if model_key in self.results:\n",
        "                    r2 = self.results[model_key]['R2']\n",
        "                    if r2 > best_r2:\n",
        "                        best_r2 = r2\n",
        "                        best_model = model_key\n",
        "\n",
        "            # 가장 높은 R2를 보인 모델의 예측값 저장\n",
        "            if best_model:\n",
        "                best_model_predictions[target] = pd.DataFrame({\n",
        "                    'y_test': self.y_test_values[target],\n",
        "                    'Predicted': self.predictions[best_model],\n",
        "                    'Best_Model': best_model\n",
        "                })\n",
        "\n",
        "        return best_model_predictions\n"
      ],
      "metadata": {
        "id": "0LzuP19pn9Mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VehiclePerformancePredictor:\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.scaler = StandardScaler()\n",
        "        self.models = {\n",
        "            'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "            'XGBoost': xgb.XGBRegressor(objective='reg:squarederror', random_state=42),\n",
        "            'LightGBM': lgb.LGBMRegressor(random_state=42),\n",
        "            'SVM': SVR()\n",
        "        }\n",
        "        self.results = {}\n",
        "        self.predictions = {}  # 예측값을 저장할 딕셔너리\n",
        "        self.y_test_values = {}  # 실제 y_test 값을 저장할 딕셔너리\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        # Split the data\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
        "            self.X, self.y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Standardize the data\n",
        "        self.X_train = self.scaler.fit_transform(self.X_train)\n",
        "        self.X_test = self.scaler.transform(self.X_test)\n",
        "\n",
        "    def train_models(self):\n",
        "        # Train machine learning models\n",
        "        for target in self.y.columns:\n",
        "            y_train_target = self.y_train[target]\n",
        "            y_test_target = self.y_test[target]\n",
        "\n",
        "            self.y_test_values[target] = y_test_target  # 실제 y_test 값 저장\n",
        "\n",
        "            for model_name, model in self.models.items():\n",
        "                model.fit(self.X_train, y_train_target)\n",
        "                y_pred = model.predict(self.X_test)\n",
        "\n",
        "                # Evaluate the model\n",
        "                r2 = r2_score(y_test_target, y_pred)\n",
        "                mse = mean_squared_error(y_test_target, y_pred)\n",
        "                mae = mean_absolute_error(y_test_target, y_pred)\n",
        "\n",
        "                # Save the model\n",
        "                with open(f'{model_name}_{target}_model.pkl', 'wb') as file:\n",
        "                    pickle.dump(model, file)\n",
        "\n",
        "                # Save the results\n",
        "                self.results[f'{model_name}_{target}'] = {\n",
        "                    'R2': r2,\n",
        "                    'MSE': mse,\n",
        "                    'MAE': mae\n",
        "                }\n",
        "\n",
        "                # Save predictions\n",
        "                self.predictions[f'{model_name}_{target}'] = y_pred  # 예측값 저장\n",
        "\n",
        "        # Train Stacking model\n",
        "        estimators = [\n",
        "            ('rf', self.models['RandomForest']),\n",
        "            ('xgb', self.models['XGBoost']),\n",
        "            ('lgbm', self.models['LightGBM']),\n",
        "            ('svm', self.models['SVM'])\n",
        "        ]\n",
        "        stacking_model = StackingRegressor(estimators=estimators, final_estimator=LinearRegression())\n",
        "\n",
        "        for target in self.y.columns:\n",
        "            y_train_target = self.y_train[target]\n",
        "            y_test_target = self.y_test[target]\n",
        "\n",
        "            stacking_model.fit(self.X_train, y_train_target)\n",
        "            y_pred = stacking_model.predict(self.X_test)\n",
        "\n",
        "            # Evaluate the model\n",
        "            r2 = r2_score(y_test_target, y_pred)\n",
        "            mse = mean_squared_error(y_test_target, y_pred)\n",
        "            mae = mean_absolute_error(y_test_target, y_pred)\n",
        "\n",
        "            # Save the model\n",
        "            with open(f'Stacking_{target}_model.pkl', 'wb') as file:\n",
        "                pickle.dump(stacking_model, file)\n",
        "\n",
        "            # Save the results\n",
        "            self.results[f'Stacking_{target}'] = {\n",
        "                'R2': r2,\n",
        "                'MSE': mse,\n",
        "                'MAE': mae\n",
        "            }\n",
        "\n",
        "            # Save predictions\n",
        "            self.predictions[f'Stacking_{target}'] = y_pred  # 예측값 저장\n",
        "\n",
        "    def train_tabnet(self):\n",
        "        # Train TabNet model\n",
        "        tabnet_model = TabNetRegressor()\n",
        "        tabnet_model.fit(\n",
        "            X_train=self.X_train, y_train=self.y_train.values,\n",
        "            eval_set=[(self.X_test, self.y_test.values)],\n",
        "            patience=10, max_epochs=100,\n",
        "            eval_metric=['rmse'],\n",
        "            batch_size=16, virtual_batch_size=8\n",
        "        )\n",
        "\n",
        "        # Save the TabNet model\n",
        "        with open('TabNet_model.pkl', 'wb') as file:\n",
        "            pickle.dump(tabnet_model, file)\n",
        "\n",
        "        # Predict and evaluate for each target\n",
        "        y_pred = tabnet_model.predict(self.X_test)\n",
        "\n",
        "        for i, target in enumerate(self.y.columns):\n",
        "            r2 = r2_score(self.y_test[target], y_pred[:, i])\n",
        "            mse = mean_squared_error(self.y_test[target], y_pred[:, i])\n",
        "            mae = mean_absolute_error(self.y_test[target], y_pred[:, i])\n",
        "\n",
        "            # Save the results\n",
        "            self.results[f'TabNet_{target}'] = {\n",
        "                'R2': r2,\n",
        "                'MSE': mse,\n",
        "                'MAE': mae\n",
        "            }\n",
        "\n",
        "            # Save predictions\n",
        "            self.predictions[f'TabNet_{target}'] = y_pred[:, i]  # 예측값 저장\n",
        "\n",
        "    def get_results(self):\n",
        "        # Convert results to DataFrame\n",
        "        results_df = pd.DataFrame(self.results).T\n",
        "        return results_df\n",
        "\n",
        "    def get_predictions(self):\n",
        "        # 실제 값과 예측 값을 DataFrame으로 반환\n",
        "        predictions_df = {}\n",
        "        for target in self.y.columns:\n",
        "            predictions_df[target] = pd.DataFrame({\n",
        "                'y_test': self.y_test_values[target],\n",
        "                'Predicted': self.predictions.get(f'TabNet_{target}', 'N/A')\n",
        "            })\n",
        "        return predictions_df\n",
        "\n",
        "    def get_best_model_predictions(self):\n",
        "        # 각 target에 대해 가장 높은 R2 성능을 보이는 모델과 그 예측값을 반환\n",
        "        best_model_predictions = {}\n",
        "        for target in self.y.columns:\n",
        "            best_r2 = -float('inf')\n",
        "            best_model = None\n",
        "\n",
        "            # 각 모델의 성능을 확인하여 가장 높은 R2를 보이는 모델을 선택\n",
        "            for model_name in ['RandomForest', 'XGBoost', 'LightGBM', 'SVM', 'Stacking', 'TabNet']:\n",
        "                model_key = f'{model_name}_{target}'\n",
        "                if model_key in self.results:\n",
        "                    r2 = self.results[model_key]['R2']\n",
        "                    if r2 > best_r2:\n",
        "                        best_r2 = r2\n",
        "                        best_model = model_key\n",
        "\n",
        "            # 가장 높은 R2를 보인 모델의 예측값 저장\n",
        "            if best_model:\n",
        "                best_model_predictions[target] = pd.DataFrame({\n",
        "                    'y_test': self.y_test_values[target],\n",
        "                    'Predicted': self.predictions[best_model],\n",
        "                    'Best_Model': best_model\n",
        "                })\n",
        "\n",
        "        return best_model_predictions"
      ],
      "metadata": {
        "id": "tYulu0G1t_wU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VehiclePerformancePredictor:\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.scaler = StandardScaler()\n",
        "        self.models = {\n",
        "            'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "            'XGBoost': xgb.XGBRegressor(objective='reg:squarederror', random_state=42),\n",
        "            'LightGBM': lgb.LGBMRegressor(random_state=42),\n",
        "            'SVM': SVR()\n",
        "        }\n",
        "        self.results = {}\n",
        "        self.predictions = {}  # 예측값을 저장할 딕셔너리\n",
        "        self.y_test_values = {}  # 실제 y_test 값을 저장할 딕셔너리\n",
        "\n",
        "    def preprocess_data(self):\n",
        "\n",
        "        self.X_train = carsim_x_train2\n",
        "        self.X_test = carsim_x_test2\n",
        "        self.y_train = carsim_y_train\n",
        "        self.y_test = carsim_y_test\n",
        "        # Split the data\n",
        "#        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
        "#            self.X, self.y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "        # Standardize the data\n",
        "        self.X_train = self.scaler.fit_transform(self.X_train)\n",
        "        self.X_test = self.scaler.transform(self.X_test)\n",
        "\n",
        "    def train_models(self):\n",
        "        # Train machine learning models\n",
        "        for target in self.y.columns:\n",
        "            y_train_target = self.y_train[target]\n",
        "            y_test_target = self.y_test[target]\n",
        "\n",
        "            self.y_test_values[target] = y_test_target  # 실제 y_test 값 저장\n",
        "\n",
        "            for model_name, model in self.models.items():\n",
        "                model.fit(self.X_train, y_train_target)\n",
        "                y_pred = model.predict(self.X_test)\n",
        "\n",
        "                # Evaluate the model\n",
        "                r2 = r2_score(y_test_target, y_pred)\n",
        "                mse = mean_squared_error(y_test_target, y_pred)\n",
        "                mae = mean_absolute_error(y_test_target, y_pred)\n",
        "\n",
        "                # Save the model\n",
        "                with open(f'{model_name}_{target}_model.pkl', 'wb') as file:\n",
        "                    pickle.dump(model, file)\n",
        "\n",
        "                # Save the results\n",
        "                self.results[f'{model_name}_{target}'] = {\n",
        "                    'R2': r2,\n",
        "                    'MSE': mse,\n",
        "                    'MAE': mae\n",
        "                }\n",
        "\n",
        "                # Save predictions\n",
        "                self.predictions[f'{model_name}_{target}'] = y_pred  # 예측값 저장\n",
        "\n",
        "        # Train Stacking model\n",
        "        estimators = [\n",
        "            ('rf', self.models['RandomForest']),\n",
        "            ('xgb', self.models['XGBoost']),\n",
        "            ('lgbm', self.models['LightGBM']),\n",
        "            ('svm', self.models['SVM'])\n",
        "        ]\n",
        "        stacking_model = StackingRegressor(estimators=estimators, final_estimator=LinearRegression())\n",
        "\n",
        "        for target in self.y.columns:\n",
        "            y_train_target = self.y_train[target]\n",
        "            y_test_target = self.y_test[target]\n",
        "\n",
        "            stacking_model.fit(self.X_train, y_train_target)\n",
        "            y_pred = stacking_model.predict(self.X_test)\n",
        "\n",
        "            # Evaluate the model\n",
        "            r2 = r2_score(y_test_target, y_pred)\n",
        "            mse = mean_squared_error(y_test_target, y_pred)\n",
        "            mae = mean_absolute_error(y_test_target, y_pred)\n",
        "\n",
        "            # Save the model\n",
        "            with open(f'Stacking_{target}_model.pkl', 'wb') as file:\n",
        "                pickle.dump(stacking_model, file)\n",
        "\n",
        "            # Save the results\n",
        "            self.results[f'Stacking_{target}'] = {\n",
        "                'R2': r2,\n",
        "                'MSE': mse,\n",
        "                'MAE': mae\n",
        "            }\n",
        "\n",
        "            # Save predictions\n",
        "            self.predictions[f'Stacking_{target}'] = y_pred  # 예측값 저장\n",
        "\n",
        "    def train_tabnet(self):\n",
        "        # Train TabNet model\n",
        "        tabnet_model = TabNetRegressor()\n",
        "        tabnet_model.fit(\n",
        "            X_train=self.X_train, y_train=self.y_train.values,\n",
        "            eval_set=[(self.X_test, self.y_test.values)],\n",
        "            patience=10, max_epochs=100,\n",
        "            eval_metric=['rmse'],\n",
        "            batch_size=16, virtual_batch_size=8\n",
        "        )\n",
        "\n",
        "        # Save the TabNet model\n",
        "        with open('TabNet_model.pkl', 'wb') as file:\n",
        "            pickle.dump(tabnet_model, file)\n",
        "\n",
        "        # Predict and evaluate for each target\n",
        "        y_pred = tabnet_model.predict(self.X_test)\n",
        "\n",
        "        for i, target in enumerate(self.y.columns):\n",
        "            r2 = r2_score(self.y_test[target], y_pred[:, i])\n",
        "            mse = mean_squared_error(self.y_test[target], y_pred[:, i])\n",
        "            mae = mean_absolute_error(self.y_test[target], y_pred[:, i])\n",
        "\n",
        "            # Save the results\n",
        "            self.results[f'TabNet_{target}'] = {\n",
        "                'R2': r2,\n",
        "                'MSE': mse,\n",
        "                'MAE': mae\n",
        "            }\n",
        "\n",
        "            # Save predictions\n",
        "            self.predictions[f'TabNet_{target}'] = y_pred[:, i]  # 예측값 저장\n",
        "\n",
        "    def get_results(self):\n",
        "        # Convert results to DataFrame\n",
        "        results_df = pd.DataFrame(self.results).T\n",
        "        return results_df\n",
        "\n",
        "    def get_predictions(self):\n",
        "        # 실제 값과 예측 값을 DataFrame으로 반환\n",
        "        predictions_df = {}\n",
        "        for target in self.y.columns:\n",
        "            predictions_df[target] = pd.DataFrame({\n",
        "                'y_test': self.y_test_values[target],\n",
        "                'Predicted': self.predictions.get(f'TabNet_{target}', 'N/A')\n",
        "            })\n",
        "        return predictions_df\n",
        "\n",
        "    def get_best_model_predictions(self):\n",
        "        # 각 target에 대해 가장 높은 R2 성능을 보이는 모델과 그 예측값을 반환\n",
        "        best_model_predictions = {}\n",
        "        for target in self.y.columns:\n",
        "            best_r2 = -float('inf')\n",
        "            best_model = None\n",
        "\n",
        "            # 각 모델의 성능을 확인하여 가장 높은 R2를 보이는 모델을 선택\n",
        "            for model_name in ['RandomForest', 'XGBoost', 'LightGBM', 'SVM', 'Stacking', 'TabNet']:\n",
        "                model_key = f'{model_name}_{target}'\n",
        "                if model_key in self.results:\n",
        "                    r2 = self.results[model_key]['R2']\n",
        "                    if r2 > best_r2:\n",
        "                        best_r2 = r2\n",
        "                        best_model = model_key\n",
        "\n",
        "            # 가장 높은 R2를 보인 모델의 예측값 저장\n",
        "            if best_model:\n",
        "                best_model_predictions[target] = pd.DataFrame({\n",
        "                    'y_test': self.y_test_values[target],\n",
        "                    'Predicted': self.predictions[best_model],\n",
        "                    'Best_Model': best_model\n",
        "                })\n",
        "\n",
        "        return best_model_predictions"
      ],
      "metadata": {
        "id": "qVBG_atSI1Lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.getcwd()"
      ],
      "metadata": {
        "id": "QQ2nkOVKve2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VehiclePerformancePredictor:\n",
        "    def __init__(self, X_new):\n",
        "        self.X_new = X_new\n",
        "        self.scaler = StandardScaler()\n",
        "        self.models = ['RandomForest', 'XGBoost', 'LightGBM', 'SVM', 'Stacking', 'TabNet']\n",
        "        self.loaded_models = {}  # 로드된 모델을 저장할 딕셔너리\n",
        "        self.predictions = {}  # 예측값을 저장할 딕셔너리\n",
        "\n",
        "    def load_models(self, target_columns):\n",
        "        # 저장된 모델 파일을 로드하는 메서드\n",
        "        for model_name in self.models:\n",
        "            for target in target_columns:\n",
        "                try:\n",
        "                    with open(f'{model_name}_{target}_model.pkl', 'rb') as file:\n",
        "                        self.loaded_models[f'{model_name}_{target}'] = pickle.load(file)\n",
        "                except FileNotFoundError:\n",
        "                    print(f\"{model_name}_{target} 모델 파일을 찾을 수 없습니다.\")\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        # 새로운 데이터를 스케일링 처리\n",
        "        self.X_new = self.scaler.fit_transform(self.X_new)\n",
        "\n",
        "    def predict_new_data(self, target_columns):\n",
        "        # 새로운 데이터에 대해 각 모델의 예측 수행\n",
        "        for target in target_columns:\n",
        "            self.predictions[target] = {}\n",
        "            for model_name in self.models:\n",
        "                model_key = f'{model_name}_{target}'\n",
        "                if model_key in self.loaded_models:\n",
        "                    model = self.loaded_models[model_key]\n",
        "                    y_pred = model.predict(self.X_new)\n",
        "                    self.predictions[target][model_name] = y_pred\n",
        "                else:\n",
        "                    self.predictions[target][model_name] = '모델이 로드되지 않았습니다.'\n",
        "        return self.predictions\n",
        "\n",
        "    def get_best_predictions(self, results):\n",
        "        # 가장 성능이 좋은 모델의 예측을 선택하는 함수\n",
        "        best_predictions = {}\n",
        "        for target, models in self.predictions.items():\n",
        "            best_model = max(results[target], key=lambda x: results[target][x]['R2'])\n",
        "            best_predictions[target] = models[best_model]\n",
        "        return best_predictions\n",
        "\n",
        "# 새로운 데이터 예측을 위한 사용 예시\n",
        "def predict_new_vehicle_performance(X_new, target_columns, results):\n",
        "    predictor = VehiclePerformancePredictor(X_new)\n",
        "\n",
        "    # 저장된 모델 로드\n",
        "    predictor.load_models(target_columns)\n",
        "\n",
        "    # 데이터 전처리\n",
        "    predictor.preprocess_data()\n",
        "\n",
        "    # 새로운 데이터에 대한 예측 수행\n",
        "    predictions = predictor.predict_new_data(target_columns)\n",
        "\n",
        "    # 가장 성능이 좋은 모델의 예측값을 선택\n",
        "    best_predictions = predictor.get_best_predictions(results)\n",
        "\n",
        "    return best_predictions\n",
        "\n",
        "# 새로운 데이터 예시\n",
        "X_new = matlab2\n",
        "\n",
        "# target 변수 명 (이전에 사용한 y의 column 명)\n",
        "target_columns = ['USG', 'SSG', 'MaxAy']\n",
        "\n",
        "# 기존의 학습된 결과 (성능 평가 결과)\n",
        "results = {\n",
        "    'USG': {\n",
        "        'RandomForest': {'R2': 0.85},\n",
        "        'XGBoost': {'R2': 0.88},\n",
        "        'LightGBM': {'R2': 0.87},\n",
        "        'SVM': {'R2': 0.80},\n",
        "        'Stacking': {'R2': 0.90},\n",
        "        'TabNet': {'R2': 0.89}\n",
        "    },\n",
        "    'SSG': {\n",
        "        'RandomForest': {'R2': 0.82},\n",
        "        'XGBoost': {'R2': 0.84},\n",
        "        'LightGBM': {'R2': 0.83},\n",
        "        'SVM': {'R2': 0.79},\n",
        "        'Stacking': {'R2': 0.86},\n",
        "        'TabNet': {'R2': 0.88}\n",
        "    },\n",
        "    'MaxAy': {\n",
        "        'RandomForest': {'R2': 0.82},\n",
        "        'XGBoost': {'R2': 0.84},\n",
        "        'LightGBM': {'R2': 0.83},\n",
        "        'SVM': {'R2': 0.79},\n",
        "        'Stacking': {'R2': 0.86},\n",
        "        'TabNet': {'R2': 0.88}\n",
        "    }\n",
        "}\n",
        "\n",
        "# 새로운 데이터에 대해 예측 수행\n",
        "best_predictions = predict_new_vehicle_performance(X_new, target_columns, results)\n",
        "\n",
        "print(best_predictions)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "uYTmfX8Hszjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mat_x = matlab2\n",
        "mat_y = matlab.loc[:, ['USG', 'SSG', 'MaxAy']]"
      ],
      "metadata": {
        "id": "yM5EoWAL8lc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib"
      ],
      "metadata": {
        "id": "EPLg9TZP89LH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = joblib.load('./LightGBM_USG_model.pkl')"
      ],
      "metadata": {
        "id": "7b83DS7X78TM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_mat = pd.concat([mat_y, mat_x], axis = 1)"
      ],
      "metadata": {
        "id": "3ZUbrOpR-VhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(mat_x, mat_y, test_size = 0.4, random_state = 42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "8tI5Nb60_eG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model.fit(X_train, y_train.iloc[:, 0])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "zB54vMRh_dcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lgbm_y_usg_pred = loaded_model.predict(X_test)"
      ],
      "metadata": {
        "id": "n9i0q781-dF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.shape, lgbm_y_usg_pred.shape"
      ],
      "metadata": {
        "id": "hj5A-1AmAhyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r2_score(lgbm_y_usg_pred, y_test.iloc[:, 0])"
      ],
      "metadata": {
        "id": "U41PiGkx62bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mean_squared_error(lgbm_y_usg_pred, y_test.iloc[:, 0]))\n",
        "print(mean_absolute_error(lgbm_y_usg_pred, y_test.iloc[:, 0]))"
      ],
      "metadata": {
        "id": "_kwrdMcMBByl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "carsim_test = carsim[(carsim['vehicle_sprung_mass'] >= 1500) & (carsim['vehicle_sprung_mass'] < 1600)]\n",
        "carsim_train = carsim[~((carsim['vehicle_sprung_mass'] >= 1500) & (carsim['vehicle_sprung_mass'] <= 1600))]\n",
        "\n",
        "carsim_x_train = carsim_train.iloc[:, 3:]\n",
        "carsim_y_train = carsim_train.iloc[:, :3]\n",
        "carsim_x_test = carsim_test.iloc[:, 3:]\n",
        "carsim_y_test = carsim_test.iloc[:, :3]"
      ],
      "metadata": {
        "id": "NqkWFHVaQNMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "carsim_x_train = carsim_trainset.iloc[:, 3:]\n",
        "carsim_y_train = carsim_trainset.iloc[:, :3]\n",
        "carsim_x_test = carsim_testset.iloc[:, 3:]\n",
        "carsim_y_test = carsim_testset.iloc[:, :3]"
      ],
      "metadata": {
        "id": "FTfruzPKFzE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "carsim_x_train['car_name_code'] = le.fit_transform(carsim_x_train['car_name'])\n",
        "carsim_x_train['car_cat_code'] = le.fit_transform(carsim_x_train['car_cat'])\n",
        "carsim_x_train['car_size_code'] = le.fit_transform(carsim_x_train['car_size'])\n",
        "carsim_x_train['tire_cat_code'] = le.fit_transform(carsim_x_train['tire_cat'])\n",
        "\n",
        "carsim_x_test['car_name_code'] = le.fit_transform(carsim_x_test['car_name'])\n",
        "carsim_x_test['car_cat_code'] = le.fit_transform(carsim_x_test['car_cat'])\n",
        "carsim_x_test['car_size_code'] = le.fit_transform(carsim_x_test['car_size'])\n",
        "carsim_x_test['tire_cat_code'] = le.fit_transform(carsim_x_test['tire_cat'])"
      ],
      "metadata": {
        "id": "Ddw06c4MabGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "carsim_x_train['car_name_code'] = le.fit_transform(carsim_x_train['car_name'])\n",
        "carsim_x_train['car_cat_code'] = le.fit_transform(carsim_x_train['car_cat'])\n",
        "carsim_x_train['car_size_code'] = le.fit_transform(carsim_x_train['car_size'])\n",
        "carsim_x_train['tire_cat_code'] = le.fit_transform(carsim_x_train['tire_cat'])\n",
        "\n",
        "carsim_x_test['car_name_code'] = le.fit_transform(carsim_x_test['car_name'])\n",
        "carsim_x_test['car_cat_code'] = le.fit_transform(carsim_x_test['car_cat'])\n",
        "carsim_x_test['car_size_code'] = le.fit_transform(carsim_x_test['car_size'])\n",
        "carsim_x_test['tire_cat_code'] = le.fit_transform(carsim_x_test['tire_cat'])"
      ],
      "metadata": {
        "id": "es3om66pF6kC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matlab['car_name_code'] = le.fit_transform(matlab['car_name'])\n",
        "matlab['car_cat_code'] = le.fit_transform(matlab['car_cat'])\n",
        "matlab['car_size_code'] = le.fit_transform(matlab['car_size'])\n",
        "matlab['tire_cat_code'] = le.fit_transform(matlab['tire_cat'])"
      ],
      "metadata": {
        "id": "08ACMiVm2DQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "carsim2 = carsim.loc[:, \"tire_number\":]"
      ],
      "metadata": {
        "id": "PezG1IXncl8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matlab2 = matlab.loc[:, \"tire_number\":]"
      ],
      "metadata": {
        "id": "ZDmO3PDF4oI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "carsim_x_train2 = carsim_x_train.loc[:, \"tire_number\":]\n",
        "carsim_x_test2 = carsim_x_test.loc[:, \"tire_number\":]"
      ],
      "metadata": {
        "id": "8dIdPeiRRFMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = carsim2\n",
        "y = carsim[['USG', 'SSG', 'MaxAy']]\n",
        "\n",
        "# 클래스 사용\n",
        "predictor = VehiclePerformancePredictor(X, y)\n",
        "predictor.preprocess_data()\n",
        "predictor.train_models()\n",
        "#predictor.train_tabnet()\n",
        "results_df = predictor.get_results()\n",
        "print(results_df)"
      ],
      "metadata": {
        "id": "Vw0rtnsjoSVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 가장 성능이 좋은 모델의 예측값 확인\n",
        "best_predictions = predictor.get_best_model_predictions()\n",
        "\n",
        "# 예측값 출력\n",
        "for target, df in best_predictions.items():\n",
        "    print(f\"Best model predictions for {target}:\")\n",
        "    print(df)\n"
      ],
      "metadata": {
        "id": "sSe9oYI-mGdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matlab[['USG', 'SSG']].loc[549]"
      ],
      "metadata": {
        "id": "FtKEZAKie-Nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "carsim.loc[609]"
      ],
      "metadata": {
        "id": "t81o2sO_f4Md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matlab"
      ],
      "metadata": {
        "id": "gmgqVAXdG02c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(x = carsim_test['vehicle_sprung_mass'], y = best_predictions['USG']['Predicted'])\n",
        "sns.scatterplot(x = carsim_test['vehicle_sprung_mass'], y = best_predictions['USG']['y_test'])\n"
      ],
      "metadata": {
        "id": "WxGVse9Nz_ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(x = carsim_test['vehicle_sprung_mass'], y = best_predictions['SSG']['Predicted'])\n",
        "sns.scatterplot(x = carsim_test['vehicle_sprung_mass'], y = best_predictions['SSG']['y_test'])"
      ],
      "metadata": {
        "id": "RgsNmunQ0lDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(x = best_predictions['USG']['y_test'], y = best_predictions['USG']['Predicted'])"
      ],
      "metadata": {
        "id": "JgJmvz7RSnMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(x = best_predictions['SSG']['y_test'], y = best_predictions['SSG']['Predicted'])"
      ],
      "metadata": {
        "id": "xjZWjxlpTApD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(x = best_predictions['MaxAy']['y_test'], y = best_predictions['MaxAy']['Predicted'])"
      ],
      "metadata": {
        "id": "cuUME3TjTEeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "usg_best = best_predictions['USG'][['y_test', 'Predicted']]\n",
        "ssg_best = best_predictions['SSG'][['y_test', 'Predicted']]\n",
        "maxay_best = best_predictions['MaxAy'][['y_test', 'Predicted']]"
      ],
      "metadata": {
        "id": "G4b71Vu4qmdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(x = usg_best['y_test'], y = usg_best['Predicted'])"
      ],
      "metadata": {
        "id": "zRlGcQUWqjv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(x = ssg_best['y_test'], y = ssg_best['Predicted'])"
      ],
      "metadata": {
        "id": "IcKLDwKrrshd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(x = maxay_best['y_test'], y = maxay_best['Predicted'])"
      ],
      "metadata": {
        "id": "zl5FoV9wrs1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df"
      ],
      "metadata": {
        "id": "E_XW6DNOl4k0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zW3mfPPF0D6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8oU8ILJu15ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uflGPmWr16yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "# 모델 초기화\n",
        "models = {\n",
        "    'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "    'XGBoost': xgb.XGBRegressor(objective='reg:squarederror', random_state=42),\n",
        "    'LightGBM': lgb.LGBMRegressor(random_state=42),\n",
        "    'SVM': SVR()\n",
        "}\n",
        "\n",
        "# 타겟 변수에 대해 각 모델 학습 및 평가\n",
        "results = {}\n",
        "\n",
        "for target in y.columns:\n",
        "    y_train_target = y_train[target]\n",
        "    y_test_target = y_test[target]\n",
        "\n",
        "    for model_name, model in models.items():\n",
        "        model.fit(X_train, y_train_target)\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # 성능 평가\n",
        "        r2 = r2_score(y_test_target, y_pred)\n",
        "        mse = mean_squared_error(y_test_target, y_pred)\n",
        "        mae = mean_absolute_error(y_test_target, y_pred)\n",
        "\n",
        "        # 결과 저장\n",
        "        results[f'{model_name}_{target}'] = {\n",
        "            'R2': r2,\n",
        "            'MSE': mse,\n",
        "            'MAE': mae\n",
        "        }\n",
        "\n",
        "# 결과 출력\n",
        "for key, metrics in results.items():\n",
        "    print(f\"Model: {key}\")\n",
        "    print(f\"R-squared: {metrics['R2']}\")\n",
        "    print(f\"MSE: {metrics['MSE']}\")\n",
        "    print(f\"MAE: {metrics['MAE']}\")\n",
        "    print(\"-\" * 30)\n"
      ],
      "metadata": {
        "id": "Fnr7Cxzd16tQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Stacking 모델 정의\n",
        "estimators = [\n",
        "    ('rf', RandomForestRegressor(n_estimators=100, random_state=42)),\n",
        "    ('xgb', xgb.XGBRegressor(objective='reg:squarederror', random_state=42)),\n",
        "    ('lgbm', lgb.LGBMRegressor(random_state=42)),\n",
        "    ('svm', SVR())\n",
        "]\n",
        "\n",
        "stacking_model = StackingRegressor(estimators=estimators, final_estimator=LinearRegression())\n",
        "\n",
        "# 타겟 변수에 대해 Stacking 모델 학습 및 평가\n",
        "for target in y.columns:\n",
        "    y_train_target = y_train[target]\n",
        "    y_test_target = y_test[target]\n",
        "\n",
        "    stacking_model.fit(X_train, y_train_target)\n",
        "    y_pred = stacking_model.predict(X_test)\n",
        "\n",
        "    # 성능 평가\n",
        "    r2 = r2_score(y_test_target, y_pred)\n",
        "    mse = mean_squared_error(y_test_target, y_pred)\n",
        "    mae = mean_absolute_error(y_test_target, y_pred)\n",
        "\n",
        "    # 결과 저장\n",
        "    results[f'Stacking_{target}'] = {\n",
        "        'R2': r2,\n",
        "        'MSE': mse,\n",
        "        'MAE': mae\n",
        "    }\n",
        "\n",
        "# 결과 출력\n",
        "for key, metrics in results.items():\n",
        "    print(f\"Model: {key}\")\n",
        "    print(f\"R-squared: {metrics['R2']}\")\n",
        "    print(f\"MSE: {metrics['MSE']}\")\n",
        "    print(f\"MAE: {metrics['MAE']}\")\n",
        "    print(\"-\" * 30)\n"
      ],
      "metadata": {
        "id": "dVSG3Ffc16qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "# 모델 초기화 (기존 코드에서 학습된 모델들이 있다고 가정)\n",
        "models = {\n",
        "    'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "    'XGBoost': xgb.XGBRegressor(objective='reg:squarederror', random_state=42),\n",
        "    'LightGBM': lgb.LGBMRegressor(random_state=42),\n",
        "    'SVM': SVR()\n",
        "}\n",
        "\n",
        "# 학습된 모델들 저장\n",
        "for model_name, model in models.items():\n",
        "    with open(f'{model_name}_model.pkl', 'wb') as file:\n",
        "        pickle.dump(model, file)\n"
      ],
      "metadata": {
        "id": "KCI6NwUh16lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 모델 불러오기\n",
        "loaded_models = {}\n",
        "for model_name in models.keys():\n",
        "    with open(f'{model_name}_model.pkl', 'rb') as file:\n",
        "        loaded_models[model_name] = pickle.load(file)\n",
        "\n",
        "# 새로운 데이터셋 (예시 데이터로 대체 가능)\n",
        "new_data = {\n",
        "    'vehicle_weight': [1550, 1650],\n",
        "    'engine_power': [120, 180],\n",
        "    'tire_width': [210, 220],\n",
        "    'tire_diameter': [16, 17]\n",
        "}\n",
        "new_df = pd.DataFrame(new_data)\n",
        "\n",
        "# 데이터 표준화 (기존 학습 시 사용한 스케일러 사용)\n",
        "scaler = StandardScaler()\n",
        "new_X = scaler.fit_transform(new_df)\n",
        "\n",
        "# 불러온 모델들을 사용하여 예측 수행\n",
        "predictions = {}\n",
        "for model_name, model in loaded_models.items():\n",
        "    predictions[model_name] = model.predict(new_X)\n",
        "\n",
        "# 결과 출력\n",
        "for model_name, prediction in predictions.items():\n",
        "    print(f\"Predictions using {model_name}: {prediction}\")\n"
      ],
      "metadata": {
        "id": "wtJMlskT2Bzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 랜덤 포레스트 모델 생성 및 학습\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# 예측 및 성능 평가\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "print(f\"Random Forest MSE: {mse_rf}\")\n"
      ],
      "metadata": {
        "id": "R5rZ--Dt2BxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# 딥러닝 모델 생성\n",
        "model = Sequential([\n",
        "    Dense(64, input_dim=X_train.shape[1], activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1)  # 출력층 (성능 예측)\n",
        "])\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# 모델 학습\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=10, validation_split=0.2)\n",
        "\n",
        "# 예측 및 성능 평가\n",
        "y_pred_dl = model.predict(X_test)\n",
        "mse_dl = mean_squared_error(y_test, y_pred_dl)\n",
        "print(f\"Deep Learning MSE: {mse_dl}\")\n"
      ],
      "metadata": {
        "id": "OBODHiv82BuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# 예시 데이터프레임 생성\n",
        "data = {\n",
        "    'vehicle_weight': [1500, 1600, 1700, 1800, 1900],\n",
        "    'engine_power': [100, 150, 200, 250, 300],\n",
        "    'tire_width': [205, 215, 225, 235, 245],\n",
        "    'tire_diameter': [15, 16, 17, 18, 19],\n",
        "    'performance_1': [10, 20, 30, 40, 50],  # target variable 1\n",
        "    'performance_2': [20, 25, 35, 45, 55],  # target variable 2\n",
        "    'performance_3': [30, 35, 40, 50, 60]   # target variable 3\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 입력 변수와 타겟 변수 분리\n",
        "X = df.drop(['performance_1', 'performance_2', 'performance_3'], axis=1)\n",
        "y = df[['performance_1', 'performance_2', 'performance_3']]\n",
        "\n",
        "# 학습 데이터와 테스트 데이터로 분리\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 데이터 표준화\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "vLpiJlH72Brf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "\n",
        "# TabNet 모델 생성\n",
        "tabnet_model = TabNetRegressor()\n",
        "\n",
        "# 모델 학습\n",
        "tabnet_model.fit(\n",
        "    X_train=X_train, y_train=y_train.values,\n",
        "    eval_set=[(X_test, y_test.values)],\n",
        "    patience=10, max_epochs=100,\n",
        "    eval_metric=['rmse'],\n",
        "    batch_size=32, virtual_batch_size=16\n",
        ")\n",
        "\n",
        "# 예측 수행\n",
        "y_pred = tabnet_model.predict(X_test)\n",
        "\n",
        "# 성능 평가\n",
        "mse = mean_squared_error(y_test, y_pred, multioutput='raw_values')\n",
        "mae = mean_absolute_error(y_test, y_pred, multioutput='raw_values')\n",
        "r2 = r2_score(y_test, y_pred, multioutput='raw_values')\n",
        "\n",
        "print(f\"TabNet Model - MSE per target: {mse}\")\n",
        "print(f\"TabNet Model - MAE per target: {mae}\")\n",
        "print(f\"TabNet Model - R2 per target: {r2}\")\n"
      ],
      "metadata": {
        "id": "DRp_8-0p2Bo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fMmQEgOM2BmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "syKVx7Fl2BjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w5Vc3Kv42Bgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HqjyGfbK2BTf"
      }
    }
  ]
}